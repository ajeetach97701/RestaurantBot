{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain, create_tagging_chain_pydantic\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import create_tool_calling_agent,AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model='gpt-3.5-turbo-0613')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = CSVLoader('./dataset/menu.csv')\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 200)\n",
    "# splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory=ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user(ask_for=['service']):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "        '''You are a smart waiter at a restaurant. First  you should ask the user in conversational ways whether the user's order is delivery or dine-in.\n",
    "        Don not ask addtional information just ask whether the order is delivery or dine-in.\n",
    "        # Example:\"Would you like to have a dine-in or delivery?\"\n",
    "        '''\n",
    "    )          \n",
    "    \n",
    "    chain = LLMChain(llm=llm, prompt=first_prompt, verbose=True,memory=memory)\n",
    "    conv = chain.run(ask_for=ask_for) \n",
    "    return conv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_user())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_order(user_query):\n",
    "    first_prompt = ChatPromptTemplate.from_template(\n",
    "       \"\"\"You are a waiter at a restaurant. Below is are some things to ask the user for in a coversation way. \n",
    "        you should only ask one question at a time even if you don't get all the info \n",
    "        don't ask as a list! First greet the user and welcome the user to our restaurant in a friendly way!\n",
    "        Explain you need take the food order from the user.\n",
    "        ### ask_for : ```food_name```\n",
    "        Current conversation:\n",
    "        {history}\n",
    "        Human: {input}\n",
    "        AI Assistant:\n",
    "        # \"\"\"\n",
    "    )\n",
    "    \n",
    "    # info_chain = LLMChain(llm = llm, prompt=first_prompt, verbose=True)\n",
    "    conversation_chain = ConversationChain(llm= llm, verbose = True,prompt = first_prompt, memory = ConversationBufferMemory(ai_prefix= \"AI Assistant\"))\n",
    "    ai_chat = conversation_chain.run(user_query)\n",
    "    return ai_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_services(user_input):\n",
    "    service_type=input(ask_user())\n",
    "\n",
    "    \n",
    "    \n",
    "    first_prompt1 = ChatPromptTemplate.from_template(\n",
    "       f\"\"\" You are a smart assistant waiter in a ABC restaurant.After the order, the user is asked whether he/she wants a dine-in or wants a delivery.Your role is mentioned below:\n",
    "       \n",
    "        If the user wants delivery then tell them to provide their delivery address(only in case of delivery) \n",
    "       \n",
    "        If the user wants dine-in then tell them to have seat and enjoy the meal. \n",
    "        In case if the user wants delivery then tell them to provide their address.\n",
    "        \n",
    "       Provide staright forward answer related to whether the order is to be deliver or dine-in and do not ask any unwanted additional information\n",
    "       \n",
    "       # ask_for :{service_type}\n",
    "      \"\"\"     \n",
    "      \n",
    "      \n",
    "    )\n",
    "\n",
    "\n",
    "    #  chain = LLMChain(llm=llm, prompt=first_prompt, verbose=True,memory=memory)\n",
    "    # conv = chain.run(ask_for=ask_for)  # Assuming a default input for now\n",
    "    # return conv\n",
    "\n",
    "    chain = LLMChain(llm = llm, prompt=first_prompt1, verbose=True,memory=memory)\n",
    "\n",
    "    convs = chain.run(ask_for=service_type)\n",
    "\n",
    "\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_services())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class address(BaseModel):\n",
    "    address:str=Field(description=\"This is the address of the customers.\")\n",
    "                                                                                              \n",
    "    #dine_in:str=Field(description=\"It is the response of the customer if they did not go with the delivery.\")\n",
    "\n",
    "\n",
    "address_chain=create_tagging_chain_pydantic(address,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_address():\n",
    "    user_input=input(ask_services())\n",
    "    chain=create_tagging_chain_pydantic(address,llm)\n",
    "    response=chain.run(user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_services():\n",
    "    service_type=input(ask_user())\n",
    "\n",
    "    \n",
    "    \n",
    "    first_prompt1 = ChatPromptTemplate.from_template(\n",
    "       f\"\"\" You are a smart assistant waiter in a ABC restaurant.After the order, the user is asked whether he/she wants a dine-in or wants a delivery.Your role is mentioned below:\n",
    "       \n",
    "        If the user wants delivery then tell them to provide their delivery address(only in case of delivery) \n",
    "       \n",
    "        If the user wants dine-in then tell them to have seat and enjoy the meal. \n",
    "        In case if the user wants delivery then tell them to provide their address.\n",
    "        \n",
    "       Provide staright forward answer related to whether the order is to be deliver or dine-in and do not ask any unwanted additional information\n",
    "       \n",
    "       # ask_for :{service_type}\n",
    "      \"\"\"     \n",
    "      \n",
    "      \n",
    "    )\n",
    "\n",
    "\n",
    "    #  chain = LLMChain(llm=llm, prompt=first_prompt, verbose=True,memory=memory)\n",
    "    # conv = chain.run(ask_for=ask_for)  # Assuming a default input for now\n",
    "    # return conv\n",
    "\n",
    "    chain = LLMChain(llm = llm, prompt=first_prompt1, verbose=True,memory=memory)\n",
    "\n",
    "    convs = chain.run(ask_for=service_type)\n",
    "\n",
    "\n",
    "    return convs\n",
    "\n",
    "address_tool=Tool(\n",
    "    name='address',\n",
    "    func=ask_services,\n",
    "    description=\"A tool that is used to record the address of a customers that is provided by the customer.\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Prompt for agent\n",
    "address_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\n",
    "        \"system\", \n",
    "        \"You are a smart assistant that record address. Use the tool for getting the address of the customers.\"\n",
    "    ),\n",
    "    (\"human\",\"{input}\"),\n",
    "    (\"placeholder\",\"{agent_scratchpad}\"),\n",
    "    (\"placeholder\",\"{tool_names}\"),\n",
    "    (\"placeholder\",\"{tools}\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [address_tool]\n",
    "agent = create_tool_calling_agent(llm, tools, address_prompt)\n",
    "agent_executor = AgentExecutor(tools = tools, agent= agent,return_intermediate_steps=False,verbose=True)\n",
    "agent_executor.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "receipt_tool = Tool(\n",
    "        name = \"ReceiptCalculator\",\n",
    "        func= calculate_total_price,\n",
    "        description = \"A tool that returns total price when the user asks for receipt or bill.\"\n",
    "    )\n",
    "# memory = ConversationBufferWindowMemory(memory_key = \"chat_history\", return_messages= True)\n",
    "prompt_receipt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"You are a helpful assistant. Make sure to use the tool for generating receipt\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "## Construct the Tools agent\n",
    "tools = [receipt_tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt_receipt)\n",
    "agent_executor = AgentExecutor(tools = tools, agent= agent)\n",
    "agent_executor.invoke({\"input\":\"I need my receipt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dine_in(BaseModel):\n",
    "    dine_in:str=Field(description=\"It is the service where user wants to have a dine-in\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
